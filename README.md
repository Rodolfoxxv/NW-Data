# ğŸ¦† NW-DATA â€“ Pipeline ETL HÃ­brido

**Categoria:** Engenharia de Dados | Pipeline ETL | Local para Nuvem

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-1.1.x-fff000?style=for-the-badge&logo=duckdb&logoColor=black)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL/Supabase-13+-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-1.x-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI/CD-2088FF?style=for-the-badge&logo=githubactions&logoColor=white)

[![GitHub stars](https://img.shields.io/github/stars/Rodolfoxxv/NW-Data?style=social)](https://github.com/Rodolfoxxv/NW-Data/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/Rodolfoxxv/NW-Data?style=social)](https://github.com/Rodolfoxxv/NW-Data/network/members)
[![GitHub issues](https://img.shields.io/github/issues/Rodolfoxxv/NW-Data)](https://github.com/Rodolfoxxv/NW-Data/issues)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI](https://github.com/Rodolfoxxv/NW-Data/actions/workflows/ci.yml/badge.svg)](https://github.com/Rodolfoxxv/NW-Data/actions/workflows/ci.yml)

[**ğŸŒ Acesse a Demo Ao Vivo**](https://rodolfoxxv-nw-data-appduckdb-app-kerutc.streamlit.app)

Pipeline completo que sincroniza tabelas do DuckDB com Postgres/Supabase e expÃµe um dashboard Streamlit para explorar o data lake. Ideal para prototipagem, demonstraÃ§Ãµes ou projetos pessoais em ambientes de ingestÃ£o controlada.

<p align="center">
  <img src="app/assets/painel.png" alt="Dashboard NW-Data - VisÃ£o Geral" width="800"/>
</p>

```mermaid
graph LR
    A[Dados Locais] --> B[DuckDB Core]
    
    subgraph "Strategy Pattern"
    B --> C{Python Loader}
    end
    
    C -- "SUPABASE" --> D[Postgres/Supabase]
    C -- "S3" --> E[AWS S3]
    C -- "DATABRICKS" --> F[Databricks Delta]
    
    style B fill:#003366,stroke:#fff,stroke-width:2px,color:#fff
    style D fill:#3e9c35,stroke:#333,color:#fff
    style E fill:#232F3E,stroke:#fff,color:#fff
    style F fill:#ff3621,stroke:#333,color:#fff
```

---

## ğŸ“‹ Ãndice
- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Tecnologias](#-tecnologias)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
  - [Desenvolvimento Local](#desenvolvimento-local)
  - [Containers Docker](#containers-docker)
- [.env e ConfiguraÃ§Ã£o](#-env-e-configuraÃ§Ã£o)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [ExecuÃ§Ã£o dos ServiÃ§os](#-execuÃ§Ã£o-dos-serviÃ§os)
- [Fluxo com Docker](#-fluxo-com-docker)
- [CI/CD](#-cicd)
- [Dados de Exemplo](#-dados-de-exemplo)
- [Testes](#-testes)
- [Troubleshooting](#-troubleshooting)
- [Roadmap](#-roadmap)
- [Como Contribuir](#-como-contribuir)
- [LicenÃ§a](#-licenÃ§a)

---

## âœ¨ Arquitetura em Camadas

O projeto implementa um pipeline de dados robusto dividido em trÃªs camadas lÃ³gicas, simulando um ambiente de produÃ§Ã£o moderno:

### 1. Camada de PreparaÃ§Ã£o (Staging Local) - **DuckDB**
Atua como um **Data Lake local** e de alta performance.
- Armazena dados brutos e estruturados antes da subida para a nuvem.
- Gerencia metadados e integridade referencial localmente.
- Permite validaÃ§Ã£o de schemas e exploraÃ§Ã£o rÃ¡pida via Dashboard sem custo de latÃªncia de rede.

### 2. Camada de IngestÃ£o & ETL - **Python Core**
O "cÃ©rebro" do sistema que conecta o local Ã  nuvem.
- **Topological Sort:** Garante que tabelas sejam carregadas na ordem correta baseada em suas dependÃªncias (FKs).
- **CDC (Change Data Capture) Simplificado:** Detecta `updated_at` para sincronizar apenas deltas (registros novos ou modificados).
- **ResiliÃªncia:** LÃ³gica de *retry* exponencial para lidar com falhas de rede.

### 3. Camada de Servir (Serving Layer) - **Supabase / Postgres**
O destino final persistente na nuvem.
- **Single Source of Truth:** Onde os dados ficam disponÃ­veis para consumo por APIs, BI ou outras aplicaÃ§Ãµes.
- Recebe dados limpos e validados, mantendo a integridade referencial do modelo original.

## ğŸ”Œ Extensibilidade Multi-Cloud (Strategy Pattern)
O projeto foi arquitetado utilizando o **PadrÃ£o Strategy**, permitindo trocar o destino dos dados apenas alterando uma variÃ¡vel de ambiente, sem refatorar o cÃ³digo.

| Destino | Status | DescriÃ§Ã£o |
| --- | --- | --- |
| **Supabase (Postgres)** | âœ… Ativo | Carga via `INSERT` otimizado (Batch) com controle transacional. |
| **AWS S3** | ğŸš€ Pronto | ExportaÃ§Ã£o direta de Parquet via DuckDB `httpfs` (Zero-Copy). |
| **Databricks** | ğŸš€ Pronto | ConexÃ£o via `databricks-sql-connector` para Delta Lake. |

---

## âš™ï¸ `.env` e ConfiguraÃ§Ã£o

| VariÃ¡vel | DescriÃ§Ã£o |
| --- | --- |
| `DESTINATION_TYPE` | Define o driver de saÃ­da: `SUPABASE` (default), `S3` ou `DATABRICKS`. |
| `DUCKDB_PATH` | Nome do arquivo DuckDB (relativo ao `DESTINATION_PATH`). |
| `DESTINATION_PATH` | DiretÃ³rio onde o arquivo DuckDB Ã© lido/grava (ex.: `./data`). |
| **Credenciais Supabase** | `DB_HOST`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`... |
| **Credenciais S3** | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `S3_BUCKET_NAME` (se `DESTINATION_TYPE=S3`). |
| **Credenciais Databricks** | `DATABRICKS_HOST`, `DATABRICKS_TOKEN`, `DATABRICKS_HTTP_PATH` (se `DESTINATION_TYPE=DATABRICKS`). |

> **Dica Windows:** se nÃ£o quiser instalar `make`, execute os comandos equivalentes (`python scripts/...` ou `streamlit run ...`) diretamente no PowerShell.

---

## ğŸ“‚ Estrutura do Projeto

```
data_lk/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ duckdb_app.py          # Dashboard Streamlit
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bootstrap_duckdb.py    # Gera as tabelas teste
â”‚   â””â”€â”€ incremental_loader.py  # Loader incremental DuckDB x Postgres  
â”œâ”€â”€ data/                      
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_incremental_loader.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env / .env.example
```

---

## â–¶ï¸ ExecuÃ§Ã£o dos ServiÃ§os

| Objetivo | Comando sugerido |
| --- | --- |
| Gerar dados de exemplo | `python scripts/bootstrap_duckdb.py` |
| Rodar loader incremental | `python scripts/incremental_loader.py` |
| Abrir dashboard Streamlit | `streamlit run app/duckdb_app.py` |
| Rodar com Make (Linux/Mac ou Windows com make) | `make bootstrap`, `make run-loader`, `make run-app` |

O loader registra cada execuÃ§Ã£o na tabela `controle_cargas`, criando tabelas no Postgres caso ainda nÃ£o existam, sincronizando PK/FK e aplicando incrementais guiados por timestamp.

---

## ğŸ³ Fluxo com Docker

1. `docker compose build` â€“ imagens atualizadas (app + loader).
2. `docker compose up -d` â€“ sobe app Streamlit (porta 8501) compartilhando `./data`.
3. `docker compose run --rm --profile loader loader` â€“ executa o loader no mesmo ambiente.
4. `docker compose down` â€“ encerra os serviÃ§os.

Os contÃªineres montam `./data`, garantindo que o arquivo DuckDB seja o mesmo do host.

---

## ğŸ”„ CI/CD

- `.github/workflows/ci.yml`: instala dependÃªncias, roda `scripts/bootstrap_duckdb.py` e executa `pytest` a cada push/PR.
- `.github/workflows/pipeline.yml`: executa o loader contra o Postgres/Supabase real ao fazer push na `main`. Recomenda-se armazenar o DuckDB como secret base64 (`DUCKDB_BASE64`) e reconstruÃ­-lo durante o job:
  ```bash
  echo "$DUCKDB_BASE64" | base64 -d > "$DESTINATION_PATH/$DUCKDB_PATH"
  ```
- Secrets adicionais como `DB_SSLMODE`/`DB_SSLROOTCERT` podem ser exportadas na etapa de â€œSet up environment variablesâ€.

---

## ğŸ—ƒï¸ Dados de Exemplo

`scripts/bootstrap_duckdb.py` cria e popula:
- `clientes`
- `pedidos`
- `itens_pedido`
- `pagamentos`
- `table_metadata` e `fk_metadata`

Executar o script Ã© seguro mesmo repetidamente: ele limpa as tabelas, recria metadados e deixa o arquivo pronto para novas demos.

---

## ğŸ§ª Testes

```bash
pytest -q
# ou
make test
```

Os testes cobrem funÃ§Ãµes crÃ­ticas como mapeamento de tipos e ordenaÃ§Ã£o topolÃ³gica das tabelas para garantir que as dependÃªncias sejam respeitadas antes da carga.

---

## ğŸ†˜ Troubleshooting

- **`ValueError: VariÃ¡veis de ambiente ausentes`** â€“ confira se `.env` existe e foi preenchido.
- **`FileNotFoundError: Arquivo DuckDB nÃ£o encontrado`** â€“ execute o bootstrap ou ajuste `DESTINATION_PATH`.
- **`psycopg2.OperationalError`** â€“ valide credenciais/IP liberado no Postgres/Supabase e, se SSL estiver habilitado, defina `DB_SSLMODE`/`DB_SSLROOTCERT`.
- **`make` nÃ£o encontrado no Windows** â€“ instale `make` via Chocolatey ou execute os comandos Python/Streamlit diretamente.

---

## ğŸ—ºï¸ Roadmap

Funcionalidades planejadas para prÃ³ximas versÃµes:

### v2.0 (Em andamento)
- [x] Strategy Pattern para mÃºltiplos destinos
- [x] Suporte a Supabase/PostgreSQL
- [x] Suporte a AWS S3 (Parquet)
- [x] Suporte a Databricks Delta Lake
- [x] Dashboard Streamlit interativo
- [x] CI/CD com GitHub Actions

### v2.1 (PrÃ³ximo)
- [ ] Particionamento automÃ¡tico para tabelas grandes
- [ ] Suporte a Google BigQuery
- [ ] MÃ©tricas de observabilidade (Prometheus/Grafana)
- [ ] Agendamento de sincronizaÃ§Ãµes (Airflow/Prefect)
- [ ] Suporte a transformaÃ§Ãµes SQL (dbt-core)

### v3.0 (Futuro)
- [ ] Interface web para gerenciamento de pipelines
- [ ] Versionamento de esquemas (data versioning)
- [ ] Suporte a streaming (Kafka/Kinesis)
- [ ] Data quality checks automÃ¡ticos
- [ ] Multi-tenancy

**SugestÃµes?** Abra uma [issue](../../issues) com a tag `enhancement`!

---

## ğŸ¤ Como Contribuir

ContribuiÃ§Ãµes sÃ£o muito bem-vindas! Este Ã© um projeto open source e a comunidade Ã© essencial para seu crescimento.

### Formas de Contribuir:
- ğŸ› Reportar bugs
- ğŸ’¡ Sugerir novas funcionalidades
- ğŸ“ Melhorar documentaÃ§Ã£o
- ğŸ”§ Implementar features
- â­ Dar uma estrela no projeto

### ComeÃ§ando:
1. Leia o [CONTRIBUTING.md](CONTRIBUTING.md) para diretrizes detalhadas
2. Confira as [issues abertas](../../issues) marcadas como `good first issue`
3. FaÃ§a um fork, desenvolva e abra um Pull Request
4. Participe das [Discussions](../../discussions)

**Todas as contribuiÃ§Ãµes serÃ£o reconhecidas!**

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT. Consulte [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ™Œ Autor

**Rodolfo Fonseca**

- ğŸ’¼ GitHub: [@RodolfoxvData](https://github.com/RodolfoxvData)
- ğŸ’¬ DÃºvidas ou colaboraÃ§Ãµes? Abra uma [issue](../../issues)

---


